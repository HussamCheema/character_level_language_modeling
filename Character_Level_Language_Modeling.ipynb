{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdKjc3xz5HTb",
        "outputId": "b7f7e923-a0c1-42a1-f36d-8937a38bcfe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Length: 1112350\n",
            "Unique Characters: 80\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "## Reading and processing text\n",
        "# https://raw.githubusercontent.com/rasbt/machine-learning-book/main/ch15/1268-0.txt\n",
        "with open('1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
        "    text=fp.read()\n",
        "\n",
        "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
        "end_indx = text.find('End of the Project Gutenberg')\n",
        "\n",
        "text = text[start_indx:end_indx]\n",
        "char_set = set(text)\n",
        "print('Total Length:', len(text))\n",
        "print('Unique Characters:', len(char_set))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_sorted = sorted(char_set)\n",
        "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
        "char_array = np.array(chars_sorted)\n",
        "\n",
        "text_encoded = np.array(\n",
        "    [char2int[ch] for ch in text],\n",
        "    dtype=np.int32)\n",
        "\n",
        "print('Text encoded shape: ', text_encoded.shape)\n",
        "\n",
        "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
        "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(char_array[text_encoded[15:21]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3Xq9fyk7eMT",
        "outputId": "a032ae5a-2824-4204-d9bf-e078de632844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text encoded shape:  (1112350,)\n",
            "THE MYSTERIOUS       == Encoding ==>  [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n",
            "[33 43 36 25 38 28]  == Reverse  ==>  ISLAND\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ex in text_encoded[:5]:\n",
        "    print('{} -> {}'.format(ex, char_array[ex]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmOrHFFY_W8E",
        "outputId": "ab4dbb9f-07b5-4cd6-ccb5-6fc23e0da254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44 -> T\n",
            "32 -> H\n",
            "29 -> E\n",
            "1 ->  \n",
            "37 -> M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 40\n",
        "chunk_size = seq_length + 1\n",
        "\n",
        "text_chunks = [text_encoded[i:i+chunk_size]\n",
        "               for i in range(len(text_encoded)-chunk_size+1)]"
      ],
      "metadata": {
        "id": "FFQwMIYh_Y_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
        "\n",
        "seq_dataset = TextDataset(torch.tensor(text_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z_vv7J781iH",
        "outputId": "39086b3d-2fe8-4ea3-be2d-cd80b824be68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-178b9785e257>:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "torch.manual_seed(1)\n",
        "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "7bmGjoqX8-_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x).unsqueeze(1)\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        return hidden.to(device), cell.to(device)\n",
        "\n",
        "vocab_size = len(char_array)\n",
        "embed_dim = 256\n",
        "rnn_hidden_size = 512\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "model = model.to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r9IP0ro9SHP",
        "outputId": "88aec457-06f1-42e3-ad01-4263b6148358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(80, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n"
      ],
      "metadata": {
        "id": "70U-Ql8D9g4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10000\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    hidden, cell = model.init_hidden(batch_size)\n",
        "    seq_batch, target_batch = next(iter(seq_dl))\n",
        "    seq_batch = seq_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    for c in range(seq_length):\n",
        "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
        "        loss += loss_fn(pred, target_batch[:, c])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss = loss.item()/seq_length\n",
        "    if epoch % 500 == 0:\n",
        "        print(f'Epoch {epoch} loss: {loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0JpOUO9B6lW",
        "outputId": "177608af-48d0-49b9-864b-c8fe9191b2b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 1.2543\n",
            "Epoch 500 loss: 1.2472\n",
            "Epoch 1000 loss: 1.2490\n",
            "Epoch 1500 loss: 1.1630\n",
            "Epoch 2000 loss: 1.2040\n",
            "Epoch 2500 loss: 1.1765\n",
            "Epoch 3000 loss: 1.1463\n",
            "Epoch 3500 loss: 1.1288\n",
            "Epoch 4000 loss: 1.1906\n",
            "Epoch 4500 loss: 1.1417\n",
            "Epoch 5000 loss: 1.0912\n",
            "Epoch 5500 loss: 1.1211\n",
            "Epoch 6000 loss: 1.1389\n",
            "Epoch 6500 loss: 1.1276\n",
            "Epoch 7000 loss: 1.0981\n",
            "Epoch 7500 loss: 1.1812\n",
            "Epoch 8000 loss: 1.1430\n",
            "Epoch 8500 loss: 1.1726\n",
            "Epoch 9000 loss: 1.1221\n",
            "Epoch 9500 loss: 1.1242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, starting_str,\n",
        "           len_generated_text=500,\n",
        "           scale_factor=2.0):\n",
        "\n",
        "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
        "\n",
        "    generated_str = starting_str\n",
        "\n",
        "    model.eval()\n",
        "    hidden, cell = model.init_hidden(1)\n",
        "    hidden = hidden.to('cpu')\n",
        "    cell = cell.to('cpu')\n",
        "    for c in range(len(starting_str)-1):\n",
        "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n",
        "\n",
        "    last_char = encoded_input[:, -1]\n",
        "    for i in range(len_generated_text):\n",
        "        logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
        "        logits = torch.squeeze(logits, 0)\n",
        "        scaled_logits = logits * scale_factor\n",
        "        m = Categorical(logits=scaled_logits)\n",
        "        last_char = m.sample()\n",
        "        generated_str += str(char_array[last_char])\n",
        "\n",
        "    return generated_str"
      ],
      "metadata": {
        "id": "Miamurf7jNL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions.categorical import Categorical\n",
        "torch.manual_seed(1)\n",
        "model.to('cpu')\n",
        "print(sample(model, starting_str='The island'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBectq7isA_y",
        "outputId": "4be1493c-42a9-40e0-e131-0024dee2d6be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The island was at\n",
            "the same times furnished up the corral, and the opening of the extremity of the lake and the sailor.\n",
            "\n",
            "â€œThe work was the sun, they would risk the operation was all the same time this was an above the sand, which explored the reporter and his companions, and the sailor had been the convicts were descended to the corral.\n",
            "\n",
            "The colonists were so possible to proceed the other part of the bodies which the surface of a shore to the Poles of the lake to the shore. It was the colonists could come \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C03lcjNIsJg-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}